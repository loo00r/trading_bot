{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "Fold 0:\n",
      "  Train: index=[0]\n",
      "  Test:  index=[1]\n",
      "Fold 1:\n",
      "  Train: index=[0 1]\n",
      "  Test:  index=[2]\n",
      "Fold 2:\n",
      "  Train: index=[0 1 2]\n",
      "  Test:  index=[3]\n",
      "Fold 3:\n",
      "  Train: index=[0 1 2 3]\n",
      "  Test:  index=[4]\n",
      "Fold 4:\n",
      "  Train: index=[0 1 2 3 4]\n",
      "  Test:  index=[5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit()\n",
    "print(tscv)\n",
    "TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 7s 63ms/step - loss: 0.1802 - val_loss: 0.1269\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.1367 - val_loss: 0.1207\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.1288 - val_loss: 0.1145\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 1s 26ms/step - loss: 0.1246 - val_loss: 0.1145\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.1180 - val_loss: 0.1071\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.1137 - val_loss: 0.1034\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 1s 24ms/step - loss: 0.1109 - val_loss: 0.1031\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.1072 - val_loss: 0.0992\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.1067 - val_loss: 0.0983\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 1s 25ms/step - loss: 0.1036 - val_loss: 0.0966\n",
      "Test Loss for Fold 1: 0.09657509624958038\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1/10\n",
      "52/52 [==============================] - 5s 38ms/step - loss: 0.1687 - val_loss: 0.1247\n",
      "Epoch 2/10\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.1276 - val_loss: 0.1142\n",
      "Epoch 3/10\n",
      "52/52 [==============================] - 1s 21ms/step - loss: 0.1180 - val_loss: 0.1082\n",
      "Epoch 4/10\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.1102 - val_loss: 0.1036\n",
      "Epoch 5/10\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.1057 - val_loss: 0.1012\n",
      "Epoch 6/10\n",
      "52/52 [==============================] - 1s 21ms/step - loss: 0.1037 - val_loss: 0.0990\n",
      "Epoch 7/10\n",
      "52/52 [==============================] - 1s 22ms/step - loss: 0.1002 - val_loss: 0.0963\n",
      "Epoch 8/10\n",
      "52/52 [==============================] - 1s 20ms/step - loss: 0.0994 - val_loss: 0.0963\n",
      "Epoch 9/10\n",
      "52/52 [==============================] - 1s 19ms/step - loss: 0.0969 - val_loss: 0.0940\n",
      "Epoch 10/10\n",
      "52/52 [==============================] - 1s 18ms/step - loss: 0.0950 - val_loss: 0.0935\n",
      "Test Loss for Fold 2: 0.0935303121805191\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1/10\n",
      "78/78 [==============================] - 6s 30ms/step - loss: 0.1440 - val_loss: 0.1135\n",
      "Epoch 2/10\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.1158 - val_loss: 0.1017\n",
      "Epoch 3/10\n",
      "78/78 [==============================] - 2s 21ms/step - loss: 0.1068 - val_loss: 0.0959\n",
      "Epoch 4/10\n",
      "78/78 [==============================] - 2s 26ms/step - loss: 0.1009 - val_loss: 0.0916\n",
      "Epoch 5/10\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0975 - val_loss: 0.0898\n",
      "Epoch 6/10\n",
      "78/78 [==============================] - 2s 20ms/step - loss: 0.0958 - val_loss: 0.0888\n",
      "Epoch 7/10\n",
      "78/78 [==============================] - 2s 24ms/step - loss: 0.0940 - val_loss: 0.0869\n",
      "Epoch 8/10\n",
      "78/78 [==============================] - 2s 25ms/step - loss: 0.0927 - val_loss: 0.0862\n",
      "Epoch 9/10\n",
      "78/78 [==============================] - 1s 17ms/step - loss: 0.0919 - val_loss: 0.0850\n",
      "Epoch 10/10\n",
      "78/78 [==============================] - 1s 18ms/step - loss: 0.0906 - val_loss: 0.0877\n",
      "Test Loss for Fold 3: 0.08766835182905197\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 5s 24ms/step - loss: 0.1411 - val_loss: 0.1179\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1100 - val_loss: 0.1085\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.1017 - val_loss: 0.1029\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0967 - val_loss: 0.1008\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0941 - val_loss: 0.0984\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0920 - val_loss: 0.0972\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0908 - val_loss: 0.0963\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0893 - val_loss: 0.0966\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 2s 15ms/step - loss: 0.0885 - val_loss: 0.0947\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 2s 16ms/step - loss: 0.0871 - val_loss: 0.0944\n",
      "Test Loss for Fold 4: 0.09439407289028168\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1/10\n",
      "129/129 [==============================] - 6s 27ms/step - loss: 0.1332 - val_loss: 0.1052\n",
      "Epoch 2/10\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 0.1058 - val_loss: 0.0952\n",
      "Epoch 3/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0985 - val_loss: 0.0914\n",
      "Epoch 4/10\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 0.0954 - val_loss: 0.0893\n",
      "Epoch 5/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0930 - val_loss: 0.0879\n",
      "Epoch 6/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0913 - val_loss: 0.0868\n",
      "Epoch 7/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0900 - val_loss: 0.0860\n",
      "Epoch 8/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0896 - val_loss: 0.0853\n",
      "Epoch 9/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0882 - val_loss: 0.0845\n",
      "Epoch 10/10\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 0.0874 - val_loss: 0.0847\n",
      "Test Loss for Fold 5: 0.08465806394815445\n",
      "\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_15876\\85512661.py\", line 106, in log_feature_importance  *\n        for i, importance in enumerate(feature_importance):\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Логування ваг\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m feature_writer\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mlog_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Запуск TensorBoard:\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo view TensorBoard logs, run:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtensorboard --logdir=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1233\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1234\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Pc\\AppData\\Local\\Temp\\ipykernel_15876\\85512661.py\", line 106, in log_feature_importance  *\n        for i, importance in enumerate(feature_importance):\n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Налаштування даних\n",
    "seq_length = 50\n",
    "pred_length = 5\n",
    "n_features = 12\n",
    "\n",
    "# Імітація датасету\n",
    "data_length = 5000  # Довжина датасету\n",
    "data = np.random.rand(data_length, n_features)  # Випадкові дані для демонстрації\n",
    "\n",
    "# Функція для створення послідовностей\n",
    "def create_sequences(data, seq_length, pred_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - pred_length + 1):\n",
    "        X.append(data[i:i+seq_length])  # Вхідна послідовність\n",
    "        y.append(data[i+seq_length:i+seq_length+pred_length, 5])  # Прогнозування \"oc_range\" (індекс 5)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Масштабування даних\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Генеруємо послідовності\n",
    "X, y = create_sequences(scaled_data, seq_length, pred_length)\n",
    "\n",
    "# Налаштування TensorBoard\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Кастомна функція втрат\n",
    "def custom_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "    return mse\n",
    "\n",
    "# Побудова LSTM моделі без Attention, з візуалізацією сили фічей\n",
    "def build_lstm_model(seq_length, n_features, pred_length, l2_value=0.0005, dropout_rate=0.2):\n",
    "    input_seq = Input(shape=(seq_length, n_features))\n",
    "    \n",
    "    # Перший LSTM шар із Dropout\n",
    "    x = LSTM(50, return_sequences=True, kernel_regularizer=l2(l2_value))(input_seq)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Другий LSTM шар із Dropout\n",
    "    x = LSTM(50, return_sequences=False, kernel_regularizer=l2(l2_value))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Вихідний шар\n",
    "    output = Dense(pred_length)(x)\n",
    "    \n",
    "    # Створення та компіляція моделі\n",
    "    model = Model(inputs=input_seq, outputs=output)\n",
    "    model.compile(optimizer='adam', loss=custom_loss)\n",
    "    return model\n",
    "\n",
    "# Використання TimeSeriesSplit\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    # Розбиття на тренувальні та тестові набори\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Побудова моделі\n",
    "    model = build_lstm_model(seq_length=seq_length, n_features=n_features, pred_length=pred_length)\n",
    "    \n",
    "    # Навчання моделі з TensorBoard\n",
    "    model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=10, \n",
    "        batch_size=32, \n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[tensorboard_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Оцінка моделі\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss for Fold {fold + 1}: {loss}\\n\")\n",
    "\n",
    "# Візуалізація сили фічей у TensorBoard\n",
    "@tf.function\n",
    "def log_feature_importance(model, X_sample):\n",
    "    \"\"\"Обчислення ваг фічей і запис у TensorBoard.\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_sample)\n",
    "        preds = model(X_sample)\n",
    "    \n",
    "    gradients = tape.gradient(preds, X_sample)\n",
    "    feature_importance = tf.reduce_mean(tf.abs(gradients), axis=(0, 1))\n",
    "    \n",
    "    # Логування ваг фічей\n",
    "    tf.summary.scalar(\"Feature Importance\", data=tf.reduce_sum(feature_importance), step=1)\n",
    "    for i, importance in enumerate(feature_importance):\n",
    "        tf.summary.scalar(f\"Feature_{i}_Importance\", data=importance, step=1)\n",
    "\n",
    "# Зразок даних для візуалізації ваг\n",
    "sample_data = tf.convert_to_tensor(X[:1], dtype=tf.float32)  # Візьмемо першу послідовність\n",
    "log_dir_features = os.path.join(\"logs\", \"features\")\n",
    "feature_writer = tf.summary.create_file_writer(log_dir_features)\n",
    "\n",
    "# Логування ваг\n",
    "with feature_writer.as_default():\n",
    "    log_feature_importance(model, sample_data)\n",
    "\n",
    "# Запуск TensorBoard:\n",
    "print(f\"To view TensorBoard logs, run:\\n\\ntensorboard --logdir={log_dir}\")\n",
    "print(f\"To view feature importance logs, run:\\n\\ntensorboard --logdir={log_dir_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def log_feature_importance(model, X_sample, writer):\n",
    "    \"\"\"Обчислення ваг фічей і запис у TensorBoard.\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(X_sample)\n",
    "        preds = model(X_sample)\n",
    "\n",
    "    gradients = tape.gradient(preds, X_sample)\n",
    "    feature_importance = tf.reduce_mean(tf.abs(gradients), axis=(0, 1))\n",
    "\n",
    "    # Запис ваг через TensorBoard\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(\"Total Feature Importance\", data=tf.reduce_sum(feature_importance), step=0)\n",
    "        for i in tf.range(tf.shape(feature_importance)[0]):\n",
    "            tf.summary.scalar(f\"Feature_{i}_Importance\", data=feature_importance[i], step=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To view feature importance logs, run:\n",
      "\n",
      "tensorboard --logdir=logs\\features\n"
     ]
    }
   ],
   "source": [
    "# Логування ваг через TensorBoard\n",
    "sample_data = tf.convert_to_tensor(X[:1], dtype=tf.float32)  # Зразок даних\n",
    "log_dir_features = os.path.join(\"logs\", \"features\")\n",
    "feature_writer = tf.summary.create_file_writer(log_dir_features)\n",
    "\n",
    "# Викликаємо функцію для логування\n",
    "log_feature_importance(model, sample_data, feature_writer)\n",
    "\n",
    "# Запуск TensorBoard\n",
    "print(f\"To view feature importance logs, run:\\n\\ntensorboard --logdir={log_dir_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
